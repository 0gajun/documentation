---
title: Configure log ingestion
type: apicontent
order: 17.2
external_redirect: /api/#config-logs
---


## Configure Pipelines & Processors

Pipelines and processors operate on incoming logs, parsing and transforming them into structured attributes for easier querying.

* See the [Pipelines Configuration Page](https://app.datadoghq.com/logs/pipelines) to see the pipelines and processors currently configured in our UI.
* See user [documentation here](https://docs.datadoghq.com/logs/processing/)


### Pipeline Order Endpoint

The pipeline order endpoints are for controlling the _order_ in which an organisation's pipelines are processed.

/!\ Logs are processed sequentially. Reordering a pipeline may change the structure and content of the data processed by other pipelines and their processors.

##### Signature

Action | Verb | Path | Payload | Response |
------ | ---- | ---- | ------- | -------- |
Get Pipeline Order | `GET` | `/api/v1/logs/config/pipeline-order` | \<none> | `[PipelineOrder]`
Update Pipeline Order | `PUT` | `/api/v1/logs/config/pipeline-order` | `[PipelineOrder]` | `[PipelineOrder]`

##### Structure

`[PipelineOrder]`
```javascript
{
  pipeline_ids: [ 'pipeline-1', 'pipeline-2', 'pipeline-3' ]
}
```

##### Errors
- The `PUT` will be rejected with:
  - `400` if the payload does not contain a `pipeline_ids` entry or if the entry is not an array of strings.
  - `422` if the `pipeline_ids` in the payload does not contain each and every id in the current pipeline order exactly once (it can not be used to add/copy/delete pipelines, nor to do partial reordering)
- `405` if users try to `POST` or `DELETE` to `/api/v1/logs/config/pipeline-order`


### Pipelines Endpoint


##### Signature

Action | Verb | Path | Payload | Response |
------ | ---- | ---- | ------- | -------- |
Get All Pipelines | `GET` | `/api/v1/logs/config/pipelines` | \<none> | `Array<[Pipeline]>` |
Create Pipeline | `POST` | `/api/v1/logs/config/pipelines` | `[Pipeline]` | `[Pipeline]` |
Read Pipeline | `GET` | `/api/v1/logs/config/pipelines/{id}` | \<none> | `[Pipeline]` |
Update Pipeline | `PUT` | `/api/v1/logs/config/pipelines/{id}` | `[Pipeline]` | `[Pipeline]` |
Delete Pipeline | `DELETE` | `/api/v1/logs/config/pipelines/{id}` | \<none> | \<none>


##### Structure

`[Pipeline]`
```javascript
{
  id: 'xoK8GRgSQYCmnyvJc4Gv5g' // [Read Only] Unique identifier -- Attributed by server.
  type: 'pipeline'             // [Read Only] For disambiguating processors and nested pipelines
  name: 'Java'                 // Name of the pipeline, for display only.
  is_enabled: true             // True if this pipeline is enabled and processing logs.
  is_read_only: true           // [Read Only] True if this pipeline can not be modified

  // Only logs that match the filter criteria will be processed by this pipeline.
  filter: {
    query: "source:java"       // A query string. For search query syntax, see:
                               // https://docs.datadoghq.com/logs/explorer/search/
  }

  // Ordered list of child processors and nested pipelines (see below for definition)
  processors: [
    {
      name: 'Parsing Java Default formats'  // Name of the processor, for display only.
      type: 'grok-parser'                   // Type of processor. See below for options.
      is_enabled: true                      // True if this processor is enabled and processing logs.
      // For the many other Processor-specific properties, see below
    }
    {
      name: 'Define timestamp as the official timestamp of the log'
      type: 'date-remapper'
      ...
    }
    {
      name: 'Define level as the official status of the log'
      type: 'status-remapper'
      ...
    }
  ]
}
```

### Processors

#### attribute-remapper


|   Parameters 	    | Values  	                                                                                  | 
|---	              |---                                                                                         	|
| sources      	    | list of source, can be a list of attributes or tags, the first existing one is used         |
| sourceType        | (Optional) defaulted to `attribute`, defines if the sources are from log attributes or tags |
| target          	| Final attribute or tag name for the first source element that exist                       	| 
| targetType        | (Optional) defaulted to `attribute`, define if the target is a log attribute or a tag       |
| preserveSource    | `true` or `false`, remove or preserve the remapped source element                           |
| overrideOnConflict| `true` or `false`, override or not the target element if already set                        |

```
type: attribute-remapper
name: ''
enabled: true
sourceType: attribute
sources:
  - dyno
target: 'dyno'
targetType: 'tag'
preserveSource: false
overrideOnConflict: false
```

#### url-parser

|   Parameters 	    | Values  	                                                                                  | 
|---	              |---                                                                                         	|
| sources      	    | list of sources attribute, the first existing one is used                                   |
| target          	| Name of the parent attribute that will contain all the extracted details from the source   	| 

```
type: url-parser
name: ''
enabled: true
sources:
  - http.url
target: http.url_details
```

#### user-agent-parser

|   Parameters 	    | Values  	                                                                                  | 
|---	              |---                                                                                         	|
| sources      	    | list of sources attribute, the first existing one is used                                   |
| target          	| Name of the parent attribute that will contain all the extracted details from the source   	| 
| encoded          	| `true` or `false`, define if the source attribute is url encoded or not                     | 


```
type: user-agent-parser
name: ''
enabled: true
sources:
  - http.useragent
target: http.useragent_details
encoded: false
```

#### category-processor

|   Parameters 	    | Values  	                                                                                  | 
|---	              |---                                                                                         	|
| categories        | list of filter and corresponding name                                                       |
| target          	| Name of the target attribute which value is defined by the matching category               	|  

```
type: category-processor
name: ''
enabled: true
categories:
  - filter:
      query: '@http.status_code:[200 TO 299]'
    name: OK
  - filter:
      query: '@http.status_code:[300 TO 399]'
    name: notice
  - filter:
      query: '@http.status_code:[400 TO 499]'
    name: warning
  - filter:
      query: '@http.status_code:[500 TO 599]'
    name: error
target: http.status_category
```

#### grok-parser

|   Parameters 	    | Values  	                                                       | 
|---	              |---                                                               |
| source            | name of the log attribute that is parsed (defaulted to `message`)|
| grok            	| list of SupportRules (optional) and Match rules                  | 

```
type: grok-parser
name: ''
enabled: true
source: message
grok:
  supportRules: |
    _date %{date("yyyy-MM-dd HH:mm:ss"):timestamp}
    _date_ms %{date("yyyy-MM-dd HH:mm:ss,SSS"):timestamp}
    _date_slf4j %{date("yyyy-MM-dd HH:mm:ss.SSS"):timestamp}
    _duration %{integer:duration}
    _thread_name %{notSpace:logger.thread_name}
    _status %{word:level}
    _logger_name %{notSpace:logger.name}
    _context %{notSpace:logger.context}
    _line %{integer:line}
  matchRules: |
    java_slf4j %{_date_slf4j}\s+\[%{_thread_name}\]\s+%{_status}\s+%{_logger_name}\s*(%{_context}\s*)?- (?>%{word:dd.trace_id} %{word:dd.span_id} - )?%{data:message}((\n|\t)%{data:error.stack})?
    
    #this is a comment
    java_log4j %{_date} %{_status}\s+%{_logger_name}:%{_line}\s+- (?>%{word:dd.trace_id} %{word:dd.span_id} - )?%{data:message}((\n|\t)%{data:error.stack})?
    
```

#### arithmetic-processor

|   Parameters 	    | Values  	                                                                 | 
|---	              |---                                                                         |
| expression        | Arithmetic operation between log attributes                                |
| target            | Name of the attribute that contains the result of the arithmetic operation |
| replaceMissing    | `true` replaces all missing element of the expression by 0, `false` skip the operation if an attribute is missing  |


```
type: arithmetic-processor
name: ''
enabled: true
expression: (attribute1 - attribute2)
target: "newattribute"
replaceMissing: false
```

#### date-remapper

|   Parameters 	    | Values  	                                                       | 
|---	              |---                                                               |
| sources           | list of source attributes, the first existing one is used        |

```
type: date-remapper
name: Define timestamp as the official timestamp of the log
enabled: true
sources:
  - timestamp
```

#### status-remapper

|   Parameters 	    | Values  	                                                       | 
|---	              |---                                                               |
| sources           | list of source attributes, the first existing one is used        |

```
type: status-remapper
name: Define level as the official status of the log
enabled: true
sources:
  - level
```

#### message-remapper

|   Parameters 	    | Values  	                                                       | 
|---	              |---                                                               |
| sources           | list of source attributes, the first existing one is used        |

```
type: message-remapper
name: Define msg as the official message of the log
enabled: true
sources:
  - msg
```

#### trace-id-remapper

|   Parameters 	    | Values  	                                                       | 
|---	              |---                                                               |
| sources           | list of source attributes, the first existing one is used        |

```
type: trace-id-remapper
name: Define dd.trace_id as the official trace id associate to this log
enabled: true
sources:
  - dd.trace_id
```

#### service-remapper

|   Parameters 	    | Values  	                                                       | 
|---	              |---                                                               |
| sources           | list of source attributes, the first existing one is used        |

```
type: service-remapper
name: Define appname as the official log service
enabled: true
sources:
  - appname
```


##### Errors
- The `POST` and `PUT` operation will be rejected with:
  - `400` if the `[Pipeline]` in the payload contains read-only values.
  - `400` if the `[Pipeline]` in the payload does not contain all user-modifiable values.
- The `DELETE` operation will be rejected with `403` if it is called on a read only pipeline.


##### Notes
- `is_read_only`: At the time of writing, this property is synonymous with "is an integration pipeline". Users can never set the `is_read_only` field themselves and it will always be false on all user-created pipelines. Integration Pipelines are created manually on the backend for each new Datadog Logs Integration. They are added (by _reference_) to an org's configuration when we detect the technology being used.
- A Pipeline can contain many Processors, and Processors are executed _in order_.
- Nested Pipelines:
  - Pipelines may also contain _Nested Pipelines (NP)_ which themselves may contain lists of ordered Processors.
  - NPs have exactly the same structure/properties as pipelines, except they themselves can *not* contain NPs in their list of processors (we only allow one level of nesting).


## Configure Exclusion Filters

https://docs.datadoghq.com/logs/logging_without_limits/#exclusion-filters

### Index-Order Endpoint

The index order endpoints are for controlling the _order_ in which an organisation's indexes are processed.

/!\ Logs are only stored in one index and they are routed sequentially through the index list to the first index with a matching filter. Re-ordering them may change the distribution of logs.

`[IndexOrder]`
```javascript
{
  index_names: [ 'index-1', 'index-2', 'index-3' ]
}
```

Action | Verb | Path | Payload | Response | Notes
------ | ---- | ---- | ------- | -------- | -----
Get Index Order | `GET` | `/api/v1/logs/config/index-order` | \<none> | `[IndexOrder]`
Update Index Order | `PUT` | `/api/v1/logs/config/index-order` | `[IndexOrder]` | `[IndexOrder]`

Errors:
- The `PUT` will be rejected with:
  - `400` if the payload does not contain an `index_names` entry or if the entry is not an array of strings.
  - `422` if the `index_names` in the payload does not contain each and every id in the current index order exactly once (it can not be used to add/copy/delete indexes, nor to do partial reordering)

### Indexes Endpoint


##### Signature

Action | Verb | Path | Payload | Response
------ | ---- | ---- | ------- | --------
Get All Indexes | `GET` | `/api/v1/logs/config/indexes` | \<none> | `Array<[Index]>` |
Read Index | `GET` | `/api/v1/logs/config/indexes/{index_name}` | \<none> | `[Index]` |
Update Index | `PUT` | `/api/v1/logs/config/indexes/{index_name}` | `[Index]` | `[Index]` |


##### Structure

`[Index]`
```javascript
{
  name: "team-a-index"          // [Read Only] Name for the index

  // Only logs that match the filter criteria will be considered for this index.
  filter: {
    query: "team:a"             // A query string. For search query syntax, see:
                                // https://docs.datadoghq.com/logs/explorer/search/
  }


  num_retention_days: 25        // [Read Only] The number of days before logs are removed from this
                                //             index.
  daily_limit: 123456789        // [Read Only] The daily limit in lines for this index.
  is_rate_limited: true         // [Read Only] True if the index is currently dropping all logs
                                //             because it has reached its daily limit.

  // Ordered list of exclusion filters to apply before saving logs in this index.
  exclusion_filters: [
    {
      name: 'Exclude almost all debug logs'  // Name of the exclusion filter, for display only.
      is_enabled: true                       // True if this exclusion filter is enabled and processing logs.

      // Only logs that match the filter criteria will be excluded by this exclusion filter.
      filter: {
        query: 'status:debug'                // A query string. For search query syntax, see:
                                             // https://docs.datadoghq.com/logs/explorer/search/
        sample_rate: 0.994                   // A rate between 0 and 1 at which to (arbitrarily)
                                             // drop logs.
      }
    },
    {
      name: 'Exclude all info logs from service a'
      is_enabled: true
      filter: { query: 'service:a status:debug', sample_rate: 1 }
    },
    ...
  ]
}
```

##### Errors

- The `PUT` operation will be rejected with:
  - `400` if the `[Index]` in the payload contains read-only values.
  - `400` if the `[Index]` in the payload does not contain all user-modifiable values.


##### Notes

- Index creation or deletion is not enabled yet trhough API. 
- An Index can contain many ExclusionFilters, and they are executed _in order_. 


---
title: S3 Archives
kind: Documentation
description: "Forward all your Datadog logs to S3 for long term storage."
---

## S3 Archives

You can configure your Datadog account to forward all the logs ingested to your own S3 bucket, so that you can keep in long-term storage all the logs that you used Datadog to aggregate together. This guide will show you how to set this up. 
 

## Create and Configure an S3 Bucket

**First**, go into your [AWS Console](https://s3.console.aws.amazon.com/s3/) and [create an S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) to send your archives to. Be careful not to make your bucket publicly readable. 

**Second**, modify your bucket to grant write-only access to the Datadog AWS user. Do this by editing your bucketâ€™s **permissions**, and setting the **bucket policy** with the following content (replace "my-bucket-name" with the name of your bucket):

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowDatadogArchivesUploader",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::727006795293:role/logs-archives-uploader"
            },
            "Action": [
                "s3:PutObject",
                "s3:PutObjectAcl",
                "s3:ListMultipartUploadParts",
                "s3:AbortMultipartUpload"
            ],
            "Resource": "arn:aws:s3:::{{MY-BUCKET-NAME}}/*"
        }
    ]
}
```

{{< img src="logs/archives/log_archives_s3_iam_policy.png" alt="IAM Policy for S3 Archives" responsive="true" popup="true">}}

**Third**, go to your [Pipelines page in your Datadog app](https://app.datadoghq.com/logs/pipelines), and select the add archive" option at the bottom.

Input your bucket name. Optionally input a prefix directory for all the content of your log archives. Save your archive, and you are finished. 

[comment]: <> ({{< img src="logs/archives/log_archives_s3_datadog_settings.png" alt="Set your S3 bucket info in Datadog" responsive="true" popup="true">}})

Starting at that moment, all the logs that Datadog ingests are enriched by your processing pipelines and subsequently forwarded to your S3 bucket for archiving. 


## Format of the S3 Archives

The log archives that Datadog forwards to your S3 bucket will be in zipped (gzip) JSON format. Under whatever prefix you indicatate (or / if there is none), the archives are stored in a directory structure that indicates on what date and at what time they were generated, like so:

    `/my/s3/prefix/2018/05/15/archive_143201.1234.7dq1a9mnSya3bFotoErfxl.json.gz`

This directory structure simplifies the process of querying your historical log archives based on their date. 

Within the zipped JSON file, each event's content is formatted as follows:

```
{
    "_id": "123456789abcdefg",
    "date": "2018-05-15T14:31:16.0002",
    "host": "i-12345abced6789efg",
    "source": "source_name",
    "service": "service_name",
    "status": "status_level",
    "message": " ... log message content ... ",
    "attributes": { ... log attributes content ... }
}
```
